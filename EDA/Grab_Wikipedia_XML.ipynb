{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T10:59:33.247836Z",
     "start_time": "2018-08-22T10:59:33.213050Z"
    }
   },
   "outputs": [],
   "source": [
    "import lxml.etree\n",
    "import urllib\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_wiki_xml(title):\n",
    "    title = title\n",
    "    params = { \"format\":\"xml\", \"action\":\"query\", \"prop\":\"revisions\", \"rvprop\":\"timestamp|user|comment|content\" }\n",
    "    params[\"titles\"] = \"API|%s\" % urllib.parse.quote(title.encode(\"utf8\"))\n",
    "    qs = \"&\".join(\"%s=%s\" % (k, v)  for k, v in params.items())\n",
    "    url = \"http://en.wikipedia.org/w/api.php?%s\" % qs\n",
    "    tree = lxml.etree.parse(urllib.request.urlopen(url))\n",
    "    revs = tree.xpath('//rev')\n",
    "    return (revs[-1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T11:30:14.474349Z",
     "start_time": "2018-08-22T11:30:14.412711Z"
    }
   },
   "outputs": [],
   "source": [
    "popular_dataframe = pd.read_csv('../data/popular_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T11:30:14.665688Z",
     "start_time": "2018-08-22T11:30:14.634904Z"
    }
   },
   "outputs": [],
   "source": [
    "popular_dataframe = popular_dataframe.loc[:, ['Category', 'Page']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T11:50:25.153385Z",
     "start_time": "2018-08-22T11:50:25.126925Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T11:50:25.478402Z",
     "start_time": "2018-08-22T11:50:25.455101Z"
    }
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T11:50:26.168901Z",
     "start_time": "2018-08-22T11:50:26.142284Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://localhost:27017/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T11:50:26.731241Z",
     "start_time": "2018-08-22T11:50:26.706662Z"
    }
   },
   "outputs": [],
   "source": [
    "wiki_db = client['popular_wiki_database']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T11:50:27.515811Z",
     "start_time": "2018-08-22T11:50:27.490109Z"
    }
   },
   "outputs": [],
   "source": [
    "collection = wiki_db['popular_wiki_database']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important note about collections (and databases) in MongoDB is that they are created lazily - none of the above commands have actually performed any operations on the MongoDB server. Collections and databases are created when the first document is inserted into them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T10:58:49.814059Z",
     "start_time": "2018-08-22T10:58:49.783082Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wiki_xml(title):\n",
    "    title = title\n",
    "    params = { \"format\":\"xml\", \"action\":\"query\", \"prop\":\"revisions\", \"rvprop\":\"timestamp|user|comment|content\" }\n",
    "    params[\"titles\"] = \"API|%s\" % urllib.parse.quote(title.encode(\"utf8\"))\n",
    "    qs = \"&\".join(\"%s=%s\" % (k, v)  for k, v in params.items())\n",
    "    url = \"http://en.wikipedia.org/w/api.php?%s\" % qs\n",
    "    tree = lxml.etree.parse(urllib.request.urlopen(url))\n",
    "    revs = tree.xpath('//rev')\n",
    "    return (revs[-1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T11:50:30.270939Z",
     "start_time": "2018-08-22T11:50:30.234076Z"
    }
   },
   "outputs": [],
   "source": [
    "pop = popular_dataframe.replace('\\xa0', ' ', regex=True)\n",
    "pop.drop_duplicates(subset='Page', inplace=True)\n",
    "pop.Page = pop.Page.astype(str, inplace=True)\n",
    "pop.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T11:56:21.746023Z",
     "start_time": "2018-08-22T11:50:31.265746Z"
    }
   },
   "outputs": [],
   "source": [
    "for category, title in zip(pop.Category, pop.Page):\n",
    "    wiki_xml = get_wiki_xml(title)\n",
    "    post = {'category': category,\n",
    "            'text': wiki_xml}\n",
    "    posts = wiki_db.posts\n",
    "    post_id = posts.insert_one(post).inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T16:11:19.683530Z",
     "start_time": "2018-08-22T16:11:15.358390Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(list(posts.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T16:11:22.969082Z",
     "start_time": "2018-08-22T16:11:20.413187Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data[data['text'] != \"\"]\n",
    "data = data[data['text'].str.contains(\"#redirect\") == False]\n",
    "data = data[data['text'].str.contains(\"may refer to:\\n\\n*\") == False]\n",
    "data = data[data['text'].str.contains(\"can refer to:\\n\") == False]\n",
    "data = data[data['text'].str.contains(\"could refer to:\\n\") == False]\n",
    "data = data[data['text'].str.contains(\"#REDIRECT\") == False]\n",
    "data = data[data['text'].str.contains(\"== Matches ==\\n:\") == False]\n",
    "data = data[data['text'].str.contains(\"{{underconstruction\") == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T19:51:39.319060Z",
     "start_time": "2018-08-22T19:51:16.962931Z"
    }
   },
   "outputs": [],
   "source": [
    "file = '../data/enwiki.observations.text_wp10.30k.tsv'\n",
    "raw_data = pd.read_csv(file, sep='\\t', header=None)\n",
    "data = pd.DataFrame(data=list(raw_data[0].apply(literal_eval)))\n",
    "data = data[:100]\n",
    "data = data[data['text'] != \"\"]\n",
    "data = data[data['text'].str.contains(\"#redirect\") == False]\n",
    "data = data[data['text'].str.contains(\"may refer to:\\n\\n*\") == False]\n",
    "data = data[data['text'].str.contains(\"can refer to:\\n\") == False]\n",
    "data = data[data['text'].str.contains(\"could refer to:\\n\") == False]\n",
    "data = data[data['text'].str.contains(\"#REDIRECT\") == False]\n",
    "data = data[data['text'].str.contains(\"== Matches ==\\n:\") == False]\n",
    "data = data[data['text'].str.contains(\"{{underconstruction\") == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T19:51:13.343224Z",
     "start_time": "2018-08-22T19:51:13.181759Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_engineered_features(raw_text):\n",
    "    cleaned_text = clean_wiki_markup(raw_text)\n",
    "    return {\n",
    "        'cleaned_text': cleaned_text,\n",
    "        'num_web_citations': find_num_web_citations(raw_text),\n",
    "        'num_book_citations': find_num_book_citations(raw_text),\n",
    "        'num_news_citations': find_num_news_citations(raw_text),\n",
    "        'num_quotes': find_num_quotes(raw_text),\n",
    "        'num_h3_headers': find_num_h3_headers(raw_text),\n",
    "        'num_internal_links': find_num_internal_links(raw_text),\n",
    "        'num_h2_headers': find_num_h2_headers(raw_text),\n",
    "        'has_infobox': find_infobox(raw_text),\n",
    "        'num_categories': find_num_categories(raw_text),\n",
    "        'num_images': find_num_images(raw_text),\n",
    "        'num_ISBN': find_num_ISBN(raw_text),\n",
    "        'num_references': find_num_references(raw_text),\n",
    "        'article_length': find_article_length(raw_text),\n",
    "        'num_difficult_words': find_num_difficult_words(cleaned_text),\n",
    "        'dale_chall_readability_score': find_dale_chall_readability_score(cleaned_text),\n",
    "        'readability_index': find_automated_readability_index(cleaned_text),\n",
    "        'linsear_write_formula': find_linsear_write_formula(cleaned_text),\n",
    "        'gunning_fog_index': find_gunning_fog_index(cleaned_text),\n",
    "        'smog_index': find_smog_index(cleaned_text),\n",
    "        'syllable_count': find_syllable_count(cleaned_text),\n",
    "        'lexicon_count': find_lexicon_count(cleaned_text),\n",
    "        'sentence_count': find_sentence_count(cleaned_text),\n",
    "        'num_footnotes': find_num_footnotes(raw_text),\n",
    "        'num_note_tags': find_num_note_tags(raw_text),\n",
    "        'num_underlines': find_num_underlines(raw_text),\n",
    "        'num_journal_citations': find_num_journal_citations(raw_text),\n",
    "        'num_about_links': find_num_about_links(raw_text),\n",
    "        'num_wikitables': find_num_wikitables(raw_text)}\n",
    "\n",
    "def get_engineered_dataframe(raw_dataframe):\n",
    "    engineered_df = pd.DataFrame(raw_dataframe['text'].apply(create_engineered_features).tolist())\n",
    "    engineered_df['label'] = raw_dataframe['label']\n",
    "    return engineered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T19:52:08.471879Z",
     "start_time": "2018-08-22T19:51:45.040841Z"
    }
   },
   "outputs": [],
   "source": [
    "engineered_df = get_engineered_dataframe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T20:08:54.930135Z",
     "start_time": "2018-08-22T20:08:54.876334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  3. , 15. , ...,  0. ,  1. ,  9.9],\n",
       "       [ 1. ,  4. , 12. , ...,  0. ,  0. ,  9.4],\n",
       "       [ 1. ,  9. ,  1. , ...,  0. ,  0. ,  8.6],\n",
       "       ...,\n",
       "       [ 0. ,  1. ,  0. , ...,  0. ,  0. ,  8.8],\n",
       "       [ 0. ,  6. ,  0. , ...,  0. ,  0. ,  9.4],\n",
       "       [ 0. ,  7. ,  1. , ...,  1. ,  1. , 10.2]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = engineered_df.loc[:, ['has_infobox','num_categories','num_images','num_ISBN','num_references','article_length',\n",
    "                'num_difficult_words','dale_chall_readability_score','readability_index','linsear_write_formula',\n",
    "                'gunning_fog_index', 'num_web_citations','num_book_citations','num_news_citations',\n",
    "                'num_quotes','num_h3_headers','num_internal_links', 'num_h2_headers', 'syllable_count',\n",
    "                'lexicon_count', 'sentence_count','num_footnotes', 'num_note_tags', 'num_underlines', 'num_journal_citations',\n",
    "                'num_about_links', 'num_wikitables', 'smog_index']].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T18:35:20.891661Z",
     "start_time": "2018-08-22T18:35:13.969129Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from textstat.textstat import textstat\n",
    "from gensim.corpora import wikicorpus\n",
    "\n",
    "def clean_wiki_markup(raw_article):\n",
    "    \"\"\" Removes Wikipedia markup from text and return cleaned text.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_article (str): Wikipedia markup text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cleaned_article (str): Cleaned Wikipedia text\n",
    "    \"\"\"\n",
    "    semi_cleaned_article = wikicorpus.filter_wiki(raw_article)\n",
    "    cleaned_article = semi_cleaned_article.replace(\"\\n\", \"\").replace(\"\\'\", \"\").replace(\"()\", \"\").replace(\"=\", \"\").replace(\"|alt\",\"\").replace(\"\\xa0\",\"\")\n",
    "    return cleaned_article\n",
    "\n",
    "\n",
    "def find_num_categories(raw_article):\n",
    "    \"\"\" Finds the estimated number of categories listed at the bottom of a Wikipedia article.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_article (str): Wikipedia markup text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number of categories listed in text\n",
    "    \"\"\"\n",
    "    return raw_article.count(\"[[Category:\")\n",
    "\n",
    "\n",
    "def find_num_images(raw_article):\n",
    "    \"\"\" Finds the estimated number of images in a Wikipedia article.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_article (str): Wikipedia markup text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number of images present in text\n",
    "    \"\"\"\n",
    "    return raw_article.count(\"[[Image:\")\n",
    "\n",
    "\n",
    "def find_num_ISBN(raw_article):\n",
    "    \"\"\" Finds the estimated number of ISBN's listed in a Wikipedia article.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_article (str): Wikipedia markup text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number of ISBN's listed in text\n",
    "    \"\"\"\n",
    "    return raw_article.count(\"ISBN\")\n",
    "\n",
    "\n",
    "def find_num_references(raw_article):\n",
    "    \"\"\" Finds the estimated number of references listed in a Wikipedia article.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_article (str): Wikipedia markup text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number of references listed in text\n",
    "    \"\"\"\n",
    "    return raw_article.count(\"</ref>\")\n",
    "\n",
    "\n",
    "def find_article_length(cleaned_article):\n",
    "    \"\"\" Finds the article length (in characters) of a Wikipedia article.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cleaned_article (str): Cleaned Wikipedia text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Article length (in characters)\n",
    "    \"\"\"\n",
    "    return len(cleaned_article)\n",
    "\n",
    "\n",
    "def find_num_difficult_words(cleaned_article):\n",
    "    \"\"\" Finds the number of difficult words in a Wikipedia article. Words are considered difficult if they do not \n",
    "        appear in a list of the 3,000 most common words that a 4th grader can understand.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cleaned_article (str): Cleaned Wikipedia text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number of 'difficult' words\n",
    "    \"\"\"\n",
    "    return textstat.difficult_words(cleaned_article)\n",
    "\n",
    "\n",
    "def find_dale_chall_readability_score(cleaned_article):\n",
    "    \"\"\" Uses the New Dale-Chall Formula to find a score that represents the grade-level of reading that characterizes the text.\n",
    "        Scores can be interpreted as:\n",
    "\n",
    "                Score              Level of Understanding\n",
    "            ____________________________________________________\n",
    "            4.9 or lower\t|   average 4th-grade student or lower\n",
    "            5.0–5.9\taverage |   5th or 6th-grade student\n",
    "            6.0–6.9\taverage |   7th or 8th-grade student\n",
    "            7.0–7.9\taverage |   9th or 10th-grade student\n",
    "            8.0–8.9\taverage |   11th or 12th-grade student\n",
    "            9.0–9.9\taverage |   13th to 15th-grade (college) student\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cleaned_article (str): Cleaned Wikipedia text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (float): Number describing the text's Dale-Chall score\n",
    "    \"\"\"\n",
    "    return textstat.dale_chall_readability_score(cleaned_article)\n",
    "\n",
    "\n",
    "def find_automated_readability_index(cleaned_article):\n",
    "    \"\"\" Uses the Automated Readability Index to calculate a score that approximates the grade level needed\n",
    "          to comprehend the text. \n",
    "\n",
    "            For example: If the score is 8, then the grade-level needed to comprehend the text is 8th. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cleaned_article (str): Cleaned Wikipedia text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (float): Number describing the Automated Readability Index \n",
    "    \"\"\"\n",
    "    return textstat.automated_readability_index(cleaned_article)\n",
    "\n",
    "\n",
    "def find_linsear_write_formula(cleaned_article):\n",
    "    \"\"\" Uses the Linsear Write Formula to calculate a score that approximates the grade level needed\n",
    "          to comprehend the text. \n",
    "\n",
    "            For example: If the score is 8, then the grade-level needed to comprehend the text is 8th. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cleaned_article (str): Cleaned Wikipedia text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (float): Number describing the Linsear Write score \n",
    "    \"\"\"\n",
    "    return textstat.linsear_write_formula(cleaned_article)\n",
    "\n",
    "\n",
    "def find_gunning_fog_index(cleaned_article):\n",
    "    \"\"\" Uses the Gunning Gog Index to calculate a score that approximates the grade level needed\n",
    "          to comprehend the text. \n",
    "\n",
    "            For example: If the score is 8, then the grade-level needed to comprehend the text is 8th. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cleaned_article (str): Cleaned Wikipedia text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (float): Number describing the Gunning Gog Index \n",
    "    \"\"\"\n",
    "    return textstat.gunning_fog(cleaned_article)\n",
    "\n",
    "\n",
    "def find_smog_index(cleaned_article):\n",
    "    \"\"\" Uses the SMOG index to calculate a score that approximates the grade level needed to comprehend the text. \n",
    "\n",
    "        For example: If the score is 8, then the grade-level needed to comprehend the text is 8th. \n",
    "\n",
    "        Texts of fewer than 30 sentences are statistically invalid, because the SMOG formula was normed on 30-sentence samples. \n",
    "        \n",
    "        textstat requires at least 3 sentences for a result.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cleaned_article (str): Cleaned Wikipedia text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (float): Number describing the Smog Index\n",
    "    \"\"\"\n",
    "    return textstat.smog_index(cleaned_article)\n",
    "\n",
    "def find_num_web_citations(raw_article):\n",
    "    \"\"\" Finds the estimated number of web citations within a Wikipedia article.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_article (str): Wikipedia markup text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number of web citations\n",
    "    \"\"\"\n",
    "    return raw_article.count(\"{{cite web\")\n",
    "\n",
    "\n",
    "def find_num_book_citations(raw_article):\n",
    "    \"\"\" Finds the estimated number of book citations within a Wikipedia article.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_article (str): Wikipedia markup text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number of book citations\n",
    "    \"\"\"\n",
    "    return raw_article.count(\"{{cite book\")\n",
    "\n",
    "\n",
    "def find_num_news_citations(raw_article):\n",
    "    \"\"\" Finds the estimated number of news citations within a Wikipedia article.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_article (str): Wikipedia markup text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number of news citations\n",
    "    \"\"\"\n",
    "    return raw_article.count(\"{{cite news\")\n",
    "\n",
    "\n",
    "def find_num_quotes(raw_article):\n",
    "    \"\"\" Finds the estimated number of quotes mentioned in a Wikipedia article.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_article (str): Wikipedia markup text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number of quotes in Wikipedia article\n",
    "    \"\"\"\n",
    "    return raw_article.count(\"{{quote\")\n",
    "\n",
    "\n",
    "def find_num_h3_headers(raw_article):\n",
    "    \"\"\" Finds the estimated number of h3 headers in a Wikipedia article.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_article (str): Wikipedia markup text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number of h3 headers in Wikipedia article\n",
    "    \"\"\"\n",
    "    return raw_article.count(\"\\n===\")\n",
    "\n",
    "\n",
    "def find_num_internal_links(raw_article):\n",
    "    \"\"\" Finds the estimated number of internal links in a Wikipedia article.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_article (str): Wikipedia markup text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number of internal links in Wikipedia article\n",
    "    \"\"\"\n",
    "    return raw_article.count(\"[[\")\n",
    "\n",
    "\n",
    "def find_num_h2_headers(raw_article):\n",
    "    \"\"\" Finds the estimated number of h2 headers in a Wikipedia article.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_article (str): Wikipedia markup text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number of h2 headers in Wikipedia article\n",
    "    \"\"\"\n",
    "    return (raw_article.count(\"\\n==\") - find_num_h3_headers(raw_article))\n",
    "\n",
    "\n",
    "def find_syllable_count(cleaned_article):\n",
    "    \"\"\" Returns the number of syllables present in text.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cleaned_article (str): Cleaned Wikipedia text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (float): Number syllables present in text\n",
    "    \"\"\"\n",
    "    return textstat.syllable_count(cleaned_article)\n",
    "\n",
    "\n",
    "def find_lexicon_count(cleaned_article):\n",
    "    \"\"\" Returns the number of words in text. \n",
    "        Optional removepunct arugment specifies whether or not to remove punctuation symbols while counting lexicons. \n",
    "        Default value for removepunct is True. This removes  punctuation before counting lexicon items.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cleaned_article (str): Cleaned Wikipedia text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number lexicon items in text\n",
    "    \"\"\" \n",
    "    return textstat.lexicon_count(cleaned_article, removepunct=True)\n",
    "\n",
    "\n",
    "def find_sentence_count(cleaned_article):\n",
    "    \"\"\" Returns the number of sentences in text. \n",
    "        Parameters\n",
    "        ----------\n",
    "        cleaned_article (str): Cleaned Wikipedia text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int): Number sentences in text\n",
    "    \"\"\" \n",
    "    return textstat.sentence_count(cleaned_article)\n",
    "\n",
    "def find_num_footnotes(raw_article):\n",
    "    \"\"\" Finds the estimated number of footnotes in a Wikipedia article.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_article (str): Wikipedia markup text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (int): Number of footnotes in Wikipedia article\n",
    "    \"\"\"\n",
    "    return raw_article.count(\"{{\")\n",
    "\n",
    "\n",
    "def find_num_note_tags(raw_article):\n",
    "    \"\"\" Finds the estimated number of note tags in a Wikipedia article.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_article (str): Wikipedia markup text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (int): Number of note tags in Wikipedia article\n",
    "    \"\"\"\n",
    "    return raw_article.count(\"{{note\")\n",
    "\n",
    "\n",
    "def find_num_bullet_points(raw_article):\n",
    "    \"\"\" Finds the estimated number of bullet points in a Wikipedia article.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_article (str): Wikipedia markup text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (int): Number of bullet points in Wikipedia article\n",
    "    \"\"\"\n",
    "    return (raw_article.count(\"*\"))\n",
    "\n",
    "\n",
    "def find_num_underlines(raw_article):\n",
    "    \"\"\" Finds the estimated number of underlines in a Wikipedia article.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_article (str): Wikipedia markup text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (int): Number of underlines in Wikipedia article\n",
    "    \"\"\"\n",
    "    return (raw_article.count(\"<u>\"))\n",
    "\n",
    "\n",
    "def find_num_journal_citations(raw_article):\n",
    "    \"\"\" Finds the estimated number of journal citations in a Wikipedia article.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_article (str): Wikipedia markup text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (int): Number of journal citations in Wikipedia article\n",
    "    \"\"\"\n",
    "    return (raw_article.count(\"{{cite journal\"))\n",
    "\n",
    "\n",
    "def find_num_about_links(raw_article):\n",
    "    \"\"\" Finds the estimated number of 'About' links in a Wikipedia article.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_article (str): Wikipedia markup text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (int): Number of 'About' links in Wikipedia article\n",
    "    \"\"\"\n",
    "    return (raw_article.count(\"{{About\"))\n",
    "\n",
    "\n",
    "def find_num_wikitables(raw_article):\n",
    "    \"\"\" Finds the estimated number of Wiki Tables in a Wikipedia article.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_article (str): Wikipedia markup text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (int): Number of Wiki Tables in Wikipedia article\n",
    "    \"\"\"\n",
    "    return (raw_article.count('class=\"wikitable'))\n",
    "\n",
    "\n",
    "def find_infobox(raw_article):\n",
    "    \"\"\" Determines if the Wikipedia article has an infobox or not.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_article (str): Wikipedia markup text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (int): 0 if no infobox. 1 if yes infobox\n",
    "    \"\"\"\n",
    "    return int('{{Infobox' in raw_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
