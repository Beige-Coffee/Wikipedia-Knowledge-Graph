{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T15:20:37.373079Z",
     "start_time": "2018-08-23T15:20:37.319725Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "from textstat.textstat import textstat\n",
    "from gensim.corpora import wikicorpus\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "# Make it pretty\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T15:21:00.035664Z",
     "start_time": "2018-08-23T15:20:39.149706Z"
    }
   },
   "outputs": [],
   "source": [
    "file = '../data/enwiki.observations.text_wp10.30k.tsv'\n",
    "raw_data = pd.read_csv(file, sep='\\t', header=None)\n",
    "data = pd.DataFrame(data=list(raw_data[0].apply(literal_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T15:21:04.072510Z",
     "start_time": "2018-08-23T15:21:00.037473Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data[data['text'] != \"\"]\n",
    "data = data[data['text'].str.contains(\"#redirect\") == False]\n",
    "data = data[data['text'].str.contains(\"may refer to:\\n\\n*\") == False]\n",
    "data = data[data['text'].str.contains(\"can refer to:\\n\") == False]\n",
    "data = data[data['text'].str.contains(\"could refer to:\\n\") == False]\n",
    "data = data[data['text'].str.contains(\"#REDIRECT\") == False]\n",
    "data = data[data['text'].str.contains(\"== Matches ==\\n:\") == False]\n",
    "data = data[data['text'].str.contains(\"{{underconstruction\") == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T15:21:05.152661Z",
     "start_time": "2018-08-23T15:21:04.075827Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = {\"stub\": 0, \"start\": 1, \"c\": 2, \"b\": 3, \"ga\": 4, \"fa\": 5} \n",
    "data[\"label\"] = data['label'].map(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T15:21:05.241700Z",
     "start_time": "2018-08-23T15:21:05.154638Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data[15000:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T15:23:06.576995Z",
     "start_time": "2018-08-23T15:23:06.516200Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_wiki_markup(raw_article):\n",
    "    semi_cleaned_article = wikicorpus.filter_wiki(raw_article)\n",
    "    cleaned_article = semi_cleaned_article.replace(\"\\n\", \"\").replace(\"\\'\", \"\").replace(\"()\", \"\").replace(\"=\", \"\").replace(\"|alt\",\"\").replace(\"\\xa0\",\"\")\n",
    "    return cleaned_article\n",
    "def find_num_categories(raw_article):\n",
    "    return raw_article.count(\"[[Category:\")\n",
    "def find_num_images(raw_article):\n",
    "    return raw_article.count(\"[[Image:\")\n",
    "def find_num_ISBN(raw_article):\n",
    "    return raw_article.count(\"ISBN\")\n",
    "def find_num_references(raw_article):\n",
    "    return raw_article.count(\"</ref>\")\n",
    "def find_article_length(cleaned_article):\n",
    "    return len(cleaned_article)\n",
    "def find_num_difficult_words(cleaned_article):\n",
    "    return textstat.difficult_words(cleaned_article)\n",
    "def find_dale_chall_readability_score(cleaned_article):\n",
    "    return textstat.dale_chall_readability_score(cleaned_article)\n",
    "def find_automated_readability_index(cleaned_article):\n",
    "    return textstat.automated_readability_index(cleaned_article)\n",
    "def find_linsear_write_formula(cleaned_article):\n",
    "    return textstat.linsear_write_formula(cleaned_article)\n",
    "def find_gunning_fog_index(cleaned_article):\n",
    "    return textstat.gunning_fog(cleaned_article)\n",
    "def find_syllable_count(cleaned_article):\n",
    "    return textstat.syllable_count(cleaned_article)\n",
    "def find_lexicon_count(cleaned_article):\n",
    "    return textstat.lexicon_count(cleaned_article, removepunct=True)\n",
    "def find_sentence_count(cleaned_article):\n",
    "    return textstat.sentence_count(cleaned_article)\n",
    "def find_smog_index(cleaned_article):\n",
    "    return textstat.smog_index(cleaned_article)\n",
    "def find_num_web_citations(raw_article):\n",
    "    return raw_article.count(\"{{cite web\")\n",
    "def find_num_book_citations(raw_article):\n",
    "    return raw_article.count(\"{{cite book\")\n",
    "def find_num_news_citations(raw_article):\n",
    "    return raw_article.count(\"{{cite news\")\n",
    "def find_num_quotes(raw_article):\n",
    "    return raw_article.count(\"quote=\")\n",
    "def find_num_h3_headers(raw_article):\n",
    "    return raw_article.count(\"\\n===\")\n",
    "def find_num_internal_links(raw_article):\n",
    "    return (raw_article.count(\"[[\") // 2)\n",
    "def find_num_h2_headers(raw_article):\n",
    "    return (raw_article.count(\"\\n==\") - find_num_h3_headers(raw_article))\n",
    "def find_num_note_tags(raw_article):\n",
    "    return raw_article.count(\"{{note\")\n",
    "def find_num_bullet_points(raw_article):\n",
    "    return (raw_article.count(\"*\"))\n",
    "def find_num_underlines(raw_article):\n",
    "    return (raw_article.count(\"<u>\"))\n",
    "def find_num_journal_citations(raw_article):\n",
    "    return (raw_article.count(\"{{cite journal\"))\n",
    "def find_num_about_links(raw_article):\n",
    "    return (raw_article.count(\"{{About\"))\n",
    "def find_num_wikitables(raw_article):\n",
    "    return (raw_article.count('class=\"wikitable'))\n",
    "def find_num_footnotes(raw_article):\n",
    "    return raw_article.count(\"{{\")\n",
    "def find_infobox(raw_article):\n",
    "    return int('{{Infobox' in raw_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T15:23:07.652959Z",
     "start_time": "2018-08-23T15:23:07.588763Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_engineered_features(raw_text):\n",
    "    cleaned_text = clean_wiki_markup(raw_text)\n",
    "    return {\n",
    "        'cleaned_text': cleaned_text,\n",
    "        'num_web_citations': find_num_web_citations(raw_text),\n",
    "        'num_book_citations': find_num_book_citations(raw_text),\n",
    "        'num_news_citations': find_num_news_citations(raw_text),\n",
    "        'num_quotes': find_num_quotes(raw_text),\n",
    "        'num_h3_headers': find_num_h3_headers(raw_text),\n",
    "        'num_internal_links': find_num_internal_links(raw_text),\n",
    "        'num_h2_headers': find_num_h2_headers(raw_text),\n",
    "        'has_infobox': find_infobox(raw_text),\n",
    "        'num_categories': find_num_categories(raw_text),\n",
    "        'num_images': find_num_images(raw_text),\n",
    "        'num_ISBN': find_num_ISBN(raw_text),\n",
    "        'num_references': find_num_references(raw_text),\n",
    "        'article_length': find_article_length(raw_text),\n",
    "        'num_difficult_words': find_num_difficult_words(cleaned_text),\n",
    "        'dale_chall_readability_score': find_dale_chall_readability_score(cleaned_text),\n",
    "        'readability_index': find_automated_readability_index(cleaned_text),\n",
    "        'linsear_write_formula': find_linsear_write_formula(cleaned_text),\n",
    "        'gunning_fog_index': find_gunning_fog_index(cleaned_text),\n",
    "        'smog_index': find_smog_index(cleaned_text),\n",
    "        'syllable_count': find_syllable_count(cleaned_text),\n",
    "        'lexicon_count': find_lexicon_count(cleaned_text),\n",
    "        'sentence_count': find_sentence_count(cleaned_text),\n",
    "        'num_footnotes': find_num_footnotes(raw_text),\n",
    "        'num_note_tags': find_num_note_tags(raw_text),\n",
    "        'num_underlines': find_num_underlines(raw_text),\n",
    "        'num_journal_citations': find_num_journal_citations(raw_text),\n",
    "        'num_about_links': find_num_about_links(raw_text),\n",
    "        'num_wikitables': find_num_wikitables(raw_text)}\n",
    "\n",
    "def get_engineered_dataframe_yes_label(raw_dataframe):\n",
    "    engineered_df = pd.DataFrame(raw_dataframe['text'].apply(create_engineered_features).tolist())\n",
    "    engineered_df['label'] = raw_dataframe['label']\n",
    "    return engineered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T16:04:59.142092Z",
     "start_time": "2018-08-23T15:23:08.844011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(DCRS): Word Count is zero cannot divide\n",
      "Error(ARI) : Sentence count is zero, cannot divide\n",
      "Error(GF): Word Count is Zero, cannot divide\n",
      "Error(DCRS): Word Count is zero cannot divide\n",
      "Error(ARI) : Sentence count is zero, cannot divide\n",
      "Error(GF): Word Count is Zero, cannot divide\n"
     ]
    }
   ],
   "source": [
    "engineered_df = get_engineered_dataframe_yes_label(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T21:44:27.173861Z",
     "start_time": "2018-08-22T20:51:19.286663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(DCRS): Word Count is zero cannot divide\n",
      "Error(DCRS): Word Count is zero cannot divide\n",
      "Error(ARI) : Sentence count is zero, cannot divide\n",
      "Error(ARI) : Sentence count is zero, cannot divide\n",
      "Error(GF): Word Count is Zero, cannot divide\n",
      "Error(GF): Word Count is Zero, cannot divide\n"
     ]
    }
   ],
   "source": [
    "data['cleaned_text'] = data['text'].apply(clean_wiki_markup)\n",
    "data['num_web_citations'] = data['text'].apply(find_num_web_citations)\n",
    "data['num_book_citations'] = data['text'].apply(find_num_book_citations)\n",
    "data['num_news_citations'] = data['text'].apply(find_num_news_citations)\n",
    "data['num_quotes'] = data['text'].apply(find_num_quotes)\n",
    "data['num_h3_headers'] = data['text'].apply(find_num_h3_headers)\n",
    "data['num_internal_links'] = data['text'].apply(find_num_internal_links)\n",
    "data['num_h2_headers'] = data['text'].apply(find_num_h2_headers)\n",
    "data['has_infobox'] = data['text'].str.contains('{{Infobox').astype(int)\n",
    "data['num_categories'] = data['text'].apply(find_num_categories)\n",
    "data['num_images'] = data['text'].apply(find_num_images)\n",
    "data['num_ISBN'] = data['text'].apply(find_num_ISBN)\n",
    "data['num_references'] = data['text'].apply(find_num_references)\n",
    "data['article_length'] = data['text'].apply(find_article_length)\n",
    "data['num_difficult_words'] = data['cleaned_text'].apply(find_num_difficult_words)\n",
    "data['dale_chall_readability_score'] = data['cleaned_text'].apply(find_dale_chall_readability_score)\n",
    "data['readability_index'] = data['cleaned_text'].apply(find_automated_readability_index)\n",
    "data['linsear_write_formula'] = data['cleaned_text'].apply(find_linsear_write_formula)\n",
    "data['gunning_fog_index'] = data['cleaned_text'].apply(find_gunning_fog_index)\n",
    "data['smog_index'] = data['cleaned_text'].apply(find_smog_index)\n",
    "data['syllable_count'] = data['cleaned_text'].apply(find_syllable_count)\n",
    "data['lexicon_count'] = data['cleaned_text'].apply(find_lexicon_count)\n",
    "data['sentence_count'] = data['cleaned_text'].apply(find_sentence_count)\n",
    "data['num_footnotes'] = data['text'].apply(find_num_footnotes)\n",
    "data['num_note_tags'] = data['text'].apply(find_num_note_tags)\n",
    "data['num_underlines'] = data['text'].apply(find_num_underlines)\n",
    "data['num_journal_citations'] = data['text'].apply(find_num_journal_citations)\n",
    "data['num_about_links'] = data['text'].apply(find_num_about_links)\n",
    "data['num_wikitables'] = data['text'].apply(find_num_wikitables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T16:05:12.740978Z",
     "start_time": "2018-08-23T16:05:12.409975Z"
    }
   },
   "outputs": [],
   "source": [
    "engineered_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T16:05:29.669052Z",
     "start_time": "2018-08-23T16:05:29.607088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_length</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>has_infobox</th>\n",
       "      <th>lexicon_count</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>num_ISBN</th>\n",
       "      <th>num_about_links</th>\n",
       "      <th>num_book_citations</th>\n",
       "      <th>...</th>\n",
       "      <th>num_quotes</th>\n",
       "      <th>num_references</th>\n",
       "      <th>num_underlines</th>\n",
       "      <th>num_web_citations</th>\n",
       "      <th>num_wikitables</th>\n",
       "      <th>readability_index</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>syllable_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [article_length, cleaned_text, dale_chall_readability_score, gunning_fog_index, has_infobox, lexicon_count, linsear_write_formula, num_ISBN, num_about_links, num_book_citations, num_categories, num_difficult_words, num_footnotes, num_h2_headers, num_h3_headers, num_images, num_internal_links, num_journal_citations, num_news_citations, num_note_tags, num_quotes, num_references, num_underlines, num_web_citations, num_wikitables, readability_index, sentence_count, smog_index, syllable_count, label]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 30 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T21:54:09.598355Z",
     "start_time": "2018-08-22T21:54:09.507624Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data.loc[:, ['has_infobox','num_categories','num_images','num_ISBN','num_references','article_length',\n",
    "                'num_difficult_words','dale_chall_readability_score','readability_index','linsear_write_formula',\n",
    "                'gunning_fog_index', 'num_web_citations','num_book_citations','num_news_citations',\n",
    "                'num_quotes','num_h3_headers','num_internal_links', 'num_h2_headers', 'syllable_count',\n",
    "                'lexicon_count', 'sentence_count','num_footnotes', 'num_note_tags', 'num_underlines', 'num_journal_citations',\n",
    "                'num_about_links', 'num_wikitables', 'smog_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T21:54:16.986876Z",
     "start_time": "2018-08-22T21:54:16.914900Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.20, random_state=910)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T21:57:18.197696Z",
     "start_time": "2018-08-22T21:57:18.143521Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xbg_model = xgb.XGBRegressor(objective ='reg:linear', learning_rate = 0.01, max_depth=10, alpha = 10, n_estimators = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T21:57:46.223450Z",
     "start_time": "2018-08-22T21:57:19.106525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
       "       max_depth=10, min_child_weight=1, missing=None, n_estimators=500,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbg_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T21:58:16.429671Z",
     "start_time": "2018-08-22T21:58:16.022269Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = xbg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T22:00:20.081097Z",
     "start_time": "2018-08-22T22:00:19.886220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.7371038304247567\n"
     ]
    }
   ],
   "source": [
    "mse = (mean_squared_error(y_test, preds))\n",
    "\n",
    "print(f\"MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T22:23:57.559972Z",
     "start_time": "2018-08-22T22:23:08.035832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7322181156009326\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500, random_state=910)\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "print(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T22:24:18.410259Z",
     "start_time": "2018-08-22T22:24:17.780161Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_filename = \"random_forest_aug22.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(rf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T22:24:29.165748Z",
     "start_time": "2018-08-22T22:24:28.915204Z"
    }
   },
   "outputs": [],
   "source": [
    "pkl_filename2 = \"XGB_aug22.pkl\"  \n",
    "with open(pkl_filename2, 'wb') as file:  \n",
    "    pickle.dump(xbg_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T22:24:31.656065Z",
     "start_time": "2018-08-22T22:24:31.394760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB_aug22.pkl                    random_forest_aug22.pkl\r\n",
      "gradient_boosted_ensemble.ipynb  random_forest_model.py\r\n",
      "hash_vectorizer_model.py         random_forest_test_pipeline.py\r\n",
      "hash_vectorizer_test_pipeline.py word_embedding_model.ipynb\r\n",
      "neural_network_model.ipynb\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
